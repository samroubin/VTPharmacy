{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A Spatio-Temporal Accessibility Analysis of Pharmacy Care in Vermont, USA\n",
    "---\n",
    "\n",
    "Extends and adapts studies *by* Kang, J. Y., A. Michels, F. Lyu, Shaohua Wang, N. Agbodo, V. L. Freeman, and Shaowen Wang. 2020. Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA. International Journal of Health Geographics 19 (1):1–17. DOI:[10.1186/s12942-020-00229-x](https://ij-healthgeographics.biomedcentral.com/articles/10.1186/s12942-020-00229-x) AND Holler, J., Burt, D., Udoh, K., & Kedron, P. (2022). Reproduction and Reanalysis of Kang et al 2020 Spatial Accessibility of COVID-19 Health Care Resources. https://doi.org/10.17605/OSF.IO/N92V3\n",
    "\n",
    "\n",
    "\n",
    "Authors: Sam Roubin, Joseph Holler, Peter Kedron\n",
    "\n",
    "Reproduction Materials Available at: https://github.com/samroubin/VTPharmacy/tree/main\n",
    "\n",
    "Created: `2024-01-14`\n",
    "Revised: `2023-01-`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Original Data\n",
    "To perform the ESFCA method, three types of data are required, as follows: (1) road network, (2) population, and (3) pharmacy information. The road network can be obtained from the [OpenStreetMap Python Library, called OSMNX](https://github.com/gboeing/osmnx). The population data is available from the US Census Bureau on the [American Community Survey]. Lastly, hospital information available on our GitHub repository: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "Import necessary libraries to run this model.\n",
    "See `environment.yml` for the library versions used for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import re\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import folium\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import IPython\n",
    "import requests\n",
    "from IPython.display import display, clear_output\n",
    "from shapely.ops import nearest_points   #for hospital_setting function\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas==2.2.0\n",
      "geopandas==0.14.2\n",
      "networkx==3.2.1\n",
      "osmnx==1.8.1\n",
      "Python version: 3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 07:53:56) [MSC v.1937 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "from IPython.display import display, clear_output\n",
    "from shapely.ops import nearest_points   #for hospital_setting function\n",
    "import warnings\n",
    "import os\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\github\\\\samroubin\\\\VTPharmacy\\\\procedure\\\\code'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\github\\\\samroubin\\\\VTPharmacy'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use to set work directory properly\n",
    "if os.path.basename(os.getcwd()) == 'code':\n",
    "    os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "### Population by town for all states in study area (VT, NH, MA, NY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in population data by town\n",
    "population_df = gpd.read_file('./data/raw/public/population/tidycensus_population.gpkg')\n",
    "population_df.head()\n",
    "\n",
    "\n",
    "# Read in metropolitan / micropolitan classifications (NECTAS)\n",
    "nectas_df =  gpd.read_file('./data/raw/public/population/nectas.csv')\n",
    "nectas_df.head()\n",
    "\n",
    "\n",
    "# Join NECTAS classifications to population data with subdivision and county FIPS\n",
    "pop_df = pd.merge(population_df, nectas_df[['fips_subdivision', 'necta', 'fips_county', 'fips_state']], on=['fips_subdivision','fips_county', 'fips_state'], how = 'left')\n",
    "pop_df.head()\n",
    "\n",
    "# Save as geopackage into public/derived\n",
    "pop_df.to_file('./data//derived/public/pop_data.gpkg', driver='GPKG')\n",
    "\n",
    "pop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Visualize Pharmacy Data\n",
    "\n",
    "This data contains the hours of operations and the number of pharmacists and pharmacy technicians staffed at each pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pharmacy data\n",
    "pharmacies = gpd.read_file('./data/raw/public/pharmacy/pharmacies.gpkg')\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pharmacies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pharmacy staffing file does not exist, download it from OSF \n",
    "if not os.path.exists(\"./data/raw/private/pharm_staffing.csv\"):\n",
    "    print(\"Loading pharmacy staffing levels\", flush=True)\n",
    "    url = ' https://osf.io/download/yvxqj/'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('./data/raw/private/pharm_staffing.csv', 'wb').write(r.content)\n",
    "   \n",
    "    \n",
    "# Load pharmacy data from OSF project\n",
    "pharm_staffing = pd.read_csv('./data/raw/private/pharm_staffing.csv')\n",
    "\n",
    "pharm_staffing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join private staffing data to pharmacies dataset (locations, hours of operations, etc.) \n",
    "pharmacies_df = pd.merge(pharmacies, pharm_staffing, on='pharmid', how = 'left')\n",
    "pharmacies_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing staffing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Plot Map of Pharmacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacies_df.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Road Network\n",
    "\n",
    "If `Vermont_Network_Buffer.graphml` does not already exist, this cell will query the road network from OpenStreetMap.  \n",
    "\n",
    "Each of the road network code blocks may take a few mintues to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading buffered Vermont road network from ./data/raw/private/osm_roads.graphml Please wait...\n",
      "Data loaded.\n",
      "CPU times: total: 41.7 s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# To create a new graph from OpenStreetMap, delete or rename the graph file (if it exists)\n",
    "# AND set OSM to True\n",
    "# This is more likely to work on a local computer than CyberGISX\n",
    "OSM = True\n",
    "\n",
    "# Define the place name for Vermont\n",
    "place_name_vermont = 'Vermont, USA'\n",
    "\n",
    "roads_path = \"./data/raw/private/osm_roads.graphml\"\n",
    "\n",
    "# if buffered street network is not saved, and OSM is preferred, generate a new graph from OpenStreetMap and save it\n",
    "if not os.path.exists(roads_path) and OSM:\n",
    "    print(\"Loading buffered Vermont road network from OpenStreetMap. Please wait... runtime may exceed 9min...\", flush=True)\n",
    "    G = ox.graph_from_place('Vermont', network_type='drive', buffer_dist=64373.8) \n",
    "    print(\"Saving road network to\", roads_path, \" Please wait...\", flush=True)\n",
    "    ox.save_graphml(G, roads_path)\n",
    "    print(\"Data saved.\")\n",
    "    \n",
    "# otherwise, if buffered street network is not saved, download graph from the OSF project\n",
    "elif not os.path.exists(roads_path):\n",
    "    print(\"Downloading buffered Vermont road network from OSF...\", flush=True)\n",
    "    url = 'https://osf.io/download/n2q73/'  \n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    print(\"Saving road network to\", roads_path, \" Please wait...\", flush=True)\n",
    "    open(roads_path, 'wb').write(r.content)\n",
    "    \n",
    "# load the saved network graph\n",
    "if os.path.exists(roads_path):\n",
    "    print(\"Loading road network from\", roads_path, \"Please wait...\", flush=True)\n",
    "    G = ox.load_graphml(roads_path) \n",
    "    print(\"Data loaded.\") \n",
    "else:\n",
    "    print(\"Error: could not load the road network from file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot the Road Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ox.plot_graph(G, node_size = 1, bgcolor = 'white', node_color = 'black', edge_color = \"#333333\", node_alpha = 0.5, edge_linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check speed limits and highway types\n",
    "\n",
    "Display all the unique speed limit values and count how many network edges (road segments) have each value.\n",
    "We will compare this to our cleaned network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577399 edges in graph\n",
      "maxspeed\n",
      "30 mph                                              48848\n",
      "40                                                  20564\n",
      "30                                                  18812\n",
      "50                                                  15082\n",
      "25 mph                                              14227\n",
      "35 mph                                              10340\n",
      "40 mph                                               9657\n",
      "55 mph                                               6035\n",
      "45 mph                                               5790\n",
      "50 mph                                               3702\n",
      "70                                                   2089\n",
      "20 mph                                               1343\n",
      "90                                                   1246\n",
      "100                                                  1005\n",
      "65 mph                                                722\n",
      "[40 mph, 30 mph]                                      470\n",
      "15 mph                                                467\n",
      "80                                                    450\n",
      "[40, 30]                                              366\n",
      "[40 mph, 50 mph]                                      306\n",
      "[45 mph, 55 mph]                                      287\n",
      "[35 mph, 30 mph]                                      282\n",
      "[40 mph, 35 mph]                                      276\n",
      "[45 mph, 40 mph]                                      201\n",
      "[40 mph, 55 mph]                                      192\n",
      "[35 mph, 50 mph]                                      188\n",
      "10 mph                                                161\n",
      "[45 mph, 30 mph]                                      161\n",
      "[45 mph, 35 mph]                                      156\n",
      "[70, 50]                                              148\n",
      "[25 mph, 30 mph]                                      132\n",
      "[25 mph, 35 mph]                                      125\n",
      "[50, 30]                                              122\n",
      "60                                                    116\n",
      "[35 mph, 55 mph]                                      101\n",
      "[45 mph, 50 mph]                                       94\n",
      "[55 mph, 30 mph]                                       94\n",
      "[50 mph, 30 mph]                                       93\n",
      "[90, 50]                                               84\n",
      "70 mph                                                 79\n",
      "20                                                     76\n",
      "[50, 40]                                               55\n",
      "[25 mph, 40 mph]                                       50\n",
      "[20 mph, 30 mph]                                       42\n",
      "25                                                     39\n",
      "[90, 70]                                               39\n",
      "[70, 100]                                              38\n",
      "[70, 90]                                               36\n",
      "[80, 50]                                               31\n",
      "15                                                     26\n",
      "[50 mph, 55 mph]                                       26\n",
      "[40 mph, 50 mph, 30 mph]                               24\n",
      "[55 mph, 50 mph]                                       24\n",
      "[70 mph, 65 mph]                                       19\n",
      "[50, 100]                                              19\n",
      "5 mph                                                  18\n",
      "[55 mph, 65 mph]                                       17\n",
      "[40 mph, 35 mph, 30 mph]                               16\n",
      "60 mph                                                 16\n",
      "[40 mph, 35 mph, 50 mph]                               15\n",
      "[35 mph, 40 mph, 50 mph]                               15\n",
      "10                                                     14\n",
      "17 mph                                                 14\n",
      "[35 mph, 50 mph, 30 mph]                               12\n",
      "[25 mph, 20 mph]                                       12\n",
      "[70, 80]                                               12\n",
      "[35 mph, 20 mph]                                       12\n",
      "[30, 15 mph]                                           11\n",
      "[80, 70]                                               10\n",
      "45                                                     10\n",
      "[45 mph, 40 mph, 50 mph]                                8\n",
      "[45 mph, 35 mph, 30 mph]                                8\n",
      "[25 mph, 40 mph, 30 mph]                                8\n",
      "[25 mph, 40 mph, 35 mph]                                8\n",
      "[25 mph, 35 mph, 30 mph]                                8\n",
      "[45 mph, 40 mph, 30 mph]                                8\n",
      "[25 mph, 45 mph]                                        8\n",
      "[45 mph, 25 mph]                                        8\n",
      "[25 mph, 50 mph]                                        8\n",
      "[70, 40]                                                7\n",
      "[15 mph, 30 mph]                                        7\n",
      "[45 mph, 40 mph, 35 mph]                                6\n",
      "[25 mph, 15 mph]                                        6\n",
      "[25 mph, 40 mph, 50 mph]                                6\n",
      "[40 mph, 55 mph, 30 mph]                                6\n",
      "[30, 20]                                                6\n",
      "[80, 100]                                               6\n",
      "30 mph;15 mph                                           6\n",
      "[80, 40]                                                5\n",
      "65                                                      5\n",
      "[45 mph, 55 mph, 30 mph]                                4\n",
      "[35 mph, 15 mph]                                        4\n",
      "[40 mph, 20 mph]                                        4\n",
      "[35 mph, 20 mph, 30 mph]                                4\n",
      "[25 mph, 5 mph]                                         4\n",
      "[55 mph, 15 mph]                                        3\n",
      "[35 mph, 40 mph, 55 mph]                                3\n",
      "5                                                       3\n",
      "[90, 100]                                               3\n",
      "[80, 50, 70]                                            3\n",
      "[40 mph, 35 mph, 55 mph]                                3\n",
      "[70, 50, 80]                                            3\n",
      "[70, 50, 90]                                            3\n",
      "[45 mph, 65 mph]                                        3\n",
      "[90, 50, 70]                                            3\n",
      "[35 mph, 55 mph, 30 mph]                                2\n",
      "[15, 15 mph, 30 mph]                                    2\n",
      "[10 mph, 35 mph]                                        2\n",
      "[70, 50, 100, 20]                                       2\n",
      "[45 mph, 40 mph, 55 mph]                                2\n",
      "[25 mph, 35 mph, 50 mph]                                2\n",
      "35                                                      2\n",
      "[50, 35]                                                2\n",
      "[50, 20]                                                2\n",
      "[45 mph, 70 mph]                                        2\n",
      "[10 mph, 40 mph, 35 mph, 30 mph]                        2\n",
      "75                                                      2\n",
      "[80, 50, 90]                                            2\n",
      "[90, 50, 80]                                            2\n",
      "[100, 65]                                               2\n",
      "[35 mph, 55 mph, 50 mph]                                2\n",
      "[45 mph, 50 mph, 30 mph]                                2\n",
      "[50, 60]                                                2\n",
      "[90, 60]                                                2\n",
      "[80, 45]                                                2\n",
      "[60, 50]                                                2\n",
      "[45 mph, 40 mph, 35 mph, 30 mph]                        2\n",
      "[45 mph, 35 mph, 50 mph]                                2\n",
      "[40 mph, 35 mph, 20 mph]                                2\n",
      "[25 mph, 35 mph, 45 mph, 40 mph, 30 mph]                2\n",
      "[25 mph, 35 mph, 15 mph, 45 mph, 40 mph, 30 mph]        2\n",
      "[25 mph, 45 mph, 40 mph, 35 mph]                        2\n",
      "[25 mph, 40 mph, 50 mph, 30 mph]                        2\n",
      "[40 mph, 15 mph, 30 mph]                                2\n",
      "[90, 65]                                                1\n",
      "[70, 50, 100, 60]                                       1\n",
      "[60, 40]                                                1\n",
      "[10 mph, 30 mph]                                        1\n",
      "[70, 50, 40]                                            1\n",
      "[45, 100]                                               1\n",
      "[70, 90, 100]                                           1\n",
      "[70, 50, 100]                                           1\n",
      "[45 mph, 20 mph]                                        1\n",
      "[70, 45]                                                1\n",
      "[55 mph, 70 mph]                                        1\n",
      "[70, 65]                                                1\n",
      "[45 mph, 55 mph, 70 mph]                                1\n",
      "[35 mph, 40 mph, 55 mph, 50 mph]                        1\n",
      "[40 mph, 15 mph]                                        1\n",
      "[40 mph, 15, 65 mph]                                    1\n",
      "[25 mph, 40 mph, 35 mph, 50 mph]                        1\n",
      "[25 mph, 35 mph, 40 mph, 50 mph]                        1\n",
      "[40 mph, 35 mph, 50 mph, 30 mph]                        1\n",
      "[35 mph, 40 mph, 50 mph, 30 mph]                        1\n",
      "[40 mph, 35 mph, 55 mph, 50 mph]                        1\n",
      "[55 mph, 60 mph]                                        1\n",
      "[25 mph, 65 mph]                                        1\n",
      "[70, 30]                                                1\n",
      "CPU times: total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Turn network edges into a geodataframe\n",
    "edges = ox.graph_to_gdfs(G, nodes=False, edges=True)\n",
    "\n",
    "# Count frequency of each speed value\n",
    "speed_values = edges['maxspeed'].value_counts()\n",
    "\n",
    "# Ouput number of edges and frequences of speed values\n",
    "print(str(len(edges)) + \" edges in graph\")\n",
    "print(speed_values.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display all the unique highway types, which are used to impute the speed limits for each category of highway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all highway types\n",
    "print(edges['highway'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OSMNx algorithm to impute missing speed limit data assumes that units are kilometers per hour unless the units are specified.\n",
    "Therefore, search for network edges containing speed limit data without units within the United States, where speed limits are defined in miles per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find edges with speed values\n",
    "unitless = edges[edges.maxspeed.notna()]\n",
    "\n",
    "# find edges with speed values containing only a number without units\n",
    "unitless = unitless[unitless.maxspeed.str.isnumeric().fillna(False)]\n",
    "\n",
    "# explore unitless edges south of Canadian border (45 degrees latitude)\n",
    "unitless[unitless.bounds['maxy']<45].explore()\n",
    "\n",
    "# 503 *More than 503 with increased buffer* road segments contain speed data without units, and these are almost exclusively in Canada.\n",
    "# It hardly seems worthwhile to fix these few segments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only *503* (different now) edges in the network contain speed limit information without units, and very few of these are located within the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process the road network\n",
    "\n",
    "Impute speed limits with `add_edge_speeds` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9.95 s\n",
      "Wall time: 10 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.multidigraph.MultiDiGraph at 0x14841514d40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ox.speed.add_edge_speeds(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate travel time with the `add_edge_travel_times` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.9 s\n",
      "Wall time: 11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.multidigraph.MultiDiGraph at 0x14841514d40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ox.speed.add_edge_travel_times(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a point geomerty to each node in the graph, to facilitate constructing catchment area polygons later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point geometries for each node in the graph, to make constructing catchment area polygons easier\n",
    "for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check speed limits\n",
    "Display all the unique speed limit values and count how many network edges (road segments) have each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577399 edges in graph\n",
      "speed_kph\n",
      "42.0     308436\n",
      "48.3      48848\n",
      "51.6      35227\n",
      "50.2      32018\n",
      "40.0      20736\n",
      "30.0      18812\n",
      "54.0      17268\n",
      "50.0      15096\n",
      "40.2      14227\n",
      "63.9      14004\n",
      "56.3      10340\n",
      "64.4       9657\n",
      "88.5       6035\n",
      "72.4       5790\n",
      "80.5       3702\n",
      "70.0       2180\n",
      "75.6       1729\n",
      "67.6       1398\n",
      "32.2       1343\n",
      "90.0       1252\n",
      "100.0      1006\n",
      "80.0        812\n",
      "104.6       722\n",
      "60.0        720\n",
      "55.2        514\n",
      "56.0        507\n",
      "68.0        483\n",
      "24.1        467\n",
      "72.0        419\n",
      "45.6        391\n",
      "35.0        370\n",
      "52.0        333\n",
      "62.1        320\n",
      "76.0        286\n",
      "64.0        281\n",
      "47.7        278\n",
      "16.1        161\n",
      "44.0        145\n",
      "48.0        137\n",
      "33.5        117\n",
      "112.7        79\n",
      "20.0         76\n",
      "57.0         72\n",
      "45.0         71\n",
      "84.0         50\n",
      "75.0         49\n",
      "25.0         45\n",
      "67.0         39\n",
      "85.0         38\n",
      "65.0         36\n",
      "61.0         27\n",
      "15.0         26\n",
      "36.0         21\n",
      "96.3         20\n",
      "108.0        19\n",
      "96.0         17\n",
      "96.6         16\n",
      "27.4         14\n",
      "10.0         14\n",
      "69.0         12\n",
      "55.0         11\n",
      "27.0         11\n",
      "59.0         10\n",
      "53.0          9\n",
      "32.0          7\n",
      "66.0          6\n",
      "53.1          6\n",
      "73.0          5\n",
      "58.0          4\n",
      "62.0          4\n",
      "88.0          3\n",
      "92.0          3\n",
      "95.0          3\n",
      "46.0          2\n",
      "82.0          2\n",
      "29.0          2\n",
      "91.0          1\n",
      "86.0          1\n",
      "77.0          1\n",
      "CPU times: total: 15.1 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Turn network edges into a geodataframe\n",
    "nodes,edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# Count frequency of each speed value\n",
    "speed_values = edges['speed_kph'].value_counts()\n",
    "\n",
    "# Ouput number of edges and frequences of speed values\n",
    "print(str(len(edges)) + \" edges in graph\")\n",
    "print(speed_values.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges['osmid'] = edges['osmid'].astype('str') \n",
    "edges['highway'] = edges['highway'].astype('str')\n",
    "edges[[\"osmid\", \"geometry\", \"highway\", \"oneway\", \"speed_kph\"]].to_file('./data/scratch/edges.shp')\n",
    "nodes.to_file('./data/scratch/nodes.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speeds have been imputed and converted to kilometers per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate a color scheme for edges based on speed\n",
    "ec = ox.plot.get_edge_colors_by_attr(G, attr=\"speed_kph\", cmap='viridis')\n",
    "\n",
    "# plot edge speeds\n",
    "fig, ax = ox.plot_graph(G, node_size=0, edge_color=ec, bgcolor=\"white\")\n",
    "\n",
    "# note: the aesthetics of this could be improved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>street_count</th>\n",
       "      <th>geometry</th>\n",
       "      <th>highway</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>osmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61538824</th>\n",
       "      <td>42.556628</td>\n",
       "      <td>-71.993666</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-71.99367 42.55663)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61540048</th>\n",
       "      <td>42.566053</td>\n",
       "      <td>-71.992658</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-71.99266 42.56605)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61540293</th>\n",
       "      <td>42.553004</td>\n",
       "      <td>-71.973578</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-71.97358 42.55300)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61541585</th>\n",
       "      <td>42.550698</td>\n",
       "      <td>-71.971501</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-71.97150 42.55070)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61542977</th>\n",
       "      <td>42.566653</td>\n",
       "      <td>-72.004361</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-72.00436 42.56665)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541693369</th>\n",
       "      <td>45.536855</td>\n",
       "      <td>-73.644975</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-73.64497 45.53686)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541693370</th>\n",
       "      <td>45.538545</td>\n",
       "      <td>-73.643906</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-73.64391 45.53854)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541693371</th>\n",
       "      <td>45.537971</td>\n",
       "      <td>-73.644245</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-73.64424 45.53797)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11544795027</th>\n",
       "      <td>45.529019</td>\n",
       "      <td>-73.629741</td>\n",
       "      <td>4</td>\n",
       "      <td>POINT (-73.62974 45.52902)</td>\n",
       "      <td>stop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11544795157</th>\n",
       "      <td>45.522451</td>\n",
       "      <td>-73.626959</td>\n",
       "      <td>3</td>\n",
       "      <td>POINT (-73.62696 45.52245)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231407 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     y          x  street_count                    geometry  \\\n",
       "osmid                                                                         \n",
       "61538824     42.556628 -71.993666             3  POINT (-71.99367 42.55663)   \n",
       "61540048     42.566053 -71.992658             3  POINT (-71.99266 42.56605)   \n",
       "61540293     42.553004 -71.973578             3  POINT (-71.97358 42.55300)   \n",
       "61541585     42.550698 -71.971501             3  POINT (-71.97150 42.55070)   \n",
       "61542977     42.566653 -72.004361             3  POINT (-72.00436 42.56665)   \n",
       "...                ...        ...           ...                         ...   \n",
       "11541693369  45.536855 -73.644975             3  POINT (-73.64497 45.53686)   \n",
       "11541693370  45.538545 -73.643906             3  POINT (-73.64391 45.53854)   \n",
       "11541693371  45.537971 -73.644245             3  POINT (-73.64424 45.53797)   \n",
       "11544795027  45.529019 -73.629741             4  POINT (-73.62974 45.52902)   \n",
       "11544795157  45.522451 -73.626959             3  POINT (-73.62696 45.52245)   \n",
       "\n",
       "            highway  ref  \n",
       "osmid                     \n",
       "61538824        NaN  NaN  \n",
       "61540048        NaN  NaN  \n",
       "61540293        NaN  NaN  \n",
       "61541585        NaN  NaN  \n",
       "61542977        NaN  NaN  \n",
       "...             ...  ...  \n",
       "11541693369     NaN  NaN  \n",
       "11541693370     NaN  NaN  \n",
       "11541693371     NaN  NaN  \n",
       "11544795027    stop  NaN  \n",
       "11544795157     NaN  NaN  \n",
       "\n",
       "[231407 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of speed limits looks accurate, with local roads at low speeds and state and federal highways at higher speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## \"Helper\" Functions\n",
    "\n",
    "These functions are called when the model is run. \n",
    "\n",
    "### pharmacy_setting\n",
    "\n",
    "Finds the nearest network node for each pharmacy.\n",
    "\n",
    "Args:\n",
    "\n",
    "* pharmacies: GeoDataFrame of pharmacies\n",
    "* G: OSMNX network\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of pharmacies with info on nearest network node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_osmid = gpd.GeoDataFrame(nodes[\"geometry\"]).reset_index()\n",
    "nodes_osmid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacies_osm = gpd.sjoin_nearest(pharmacies_df, nodes_osmid, distance_col=\"distances\")\n",
    "\n",
    "#rename column from osmid to nearest_osm, so that it works with other code\n",
    "pharmacies_osm = pharmacies_osm.rename(columns={\"osmid\": \"nearest_osm\"})\n",
    "pharmacies_osm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### djikstra_cca_polygons\n",
    "\n",
    "Function written by Joe Holler + Derrick Burt. A more efficient way to calculate distance-weighted catchment areas for each hospital in Holler et al. (2022). We use this method to calculate catchment areas for each pharmacy.  First, create a dictionary (with a node and its corresponding drive time from the hospital) of all nodes within a 30 minute drive time (using networkx single_cource_dijkstra_path_length function). From here, two more dictionaries are constructed by querying the original one. From these dictionaries, single part convex hulls are created for each drive time interval and appended into a single list (one list with 3 polygon geometries). Within the list, the polygons are differenced from each other to produce three catchment areas.\n",
    "\n",
    "Args:\n",
    "* G: cleaned network graph *with node point geometries attached*\n",
    "* nearest_osm: A unique nearest node ID calculated for a single pharmacy\n",
    "* distances: 3 distances (in drive time) to calculate catchment areas from\n",
    "\n",
    "Returns:\n",
    "* A list of 3 differenced (not-overlapping) catchment area polygons (10 min poly, 20 min poly, 30 min poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra_cca_polygons(G, nearest_osm, distances):\n",
    "    \n",
    "## Distance_unit is given in seconds ##\n",
    "\n",
    "    ## CREATE DICTIONARIES ##\n",
    "    # create dictionary of nearest nodes\n",
    "    nearest_nodes_30 = nx.single_source_dijkstra_path_length(G, nearest_osm, distances[2], \"travel_time\") # creating the largest graph from which 10 and 20 minute drive times can be extracted from\n",
    "\n",
    "    # extract values within 20 and 10 (respectively) minutes drive times\n",
    "    nearest_nodes_20 = dict()\n",
    "    nearest_nodes_10 = dict()\n",
    "    for key, value in nearest_nodes_30.items():\n",
    "        if value <= distances[1]:\n",
    "            nearest_nodes_20[key] = value\n",
    "        if value <= distances[0]:\n",
    "            nearest_nodes_10[key] = value\n",
    "\n",
    "    ## CREATE POLYGONS FOR 3 DISTANCE CATEGORIES (10 min, 20 min, 30 min) ##\n",
    "\n",
    "    # 30 MIN\n",
    "    # If the graph already has a geometry attribute with point data,\n",
    "    # this line will create a GeoPandas GeoDataFrame from the nearest_nodes_30 dictionary\n",
    "    points_30 = gpd.GeoDataFrame(gpd.GeoSeries(nx.get_node_attributes(G.subgraph(nearest_nodes_30), 'geometry')))\n",
    "\n",
    "    # This line converts the nearest_nodes_30 dictionary into a Pandas data frame and joins it to points\n",
    "    # left_index=True and right_index=True are options for merge() to join on the index values\n",
    "    points_30 = points_30.merge(pd.Series(nearest_nodes_30).to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "    # Re-name the columns and set the geodataframe geometry to the geometry column\n",
    "    points_30 = points_30.rename(columns={'0_x':'geometry','0_y':'z'}).set_geometry('geometry')\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_30 = gpd.GeoDataFrame(gpd.GeoSeries(points_30.unary_union.convex_hull))\n",
    "    polygon_30 = polygon_30.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "\n",
    "    # 20 MIN # 1200 seconds!\n",
    "    # Select nodes less than or equal to 20\n",
    "    points_20 = points_30.query(\"z <= 1200\")\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_20 = gpd.GeoDataFrame(gpd.GeoSeries(points_20.unary_union.convex_hull))\n",
    "    polygon_20 = polygon_20.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "\n",
    "    # 10 MIN # 600 seconds!\n",
    "    # Select nodes less than or equal to 10\n",
    "    points_10 = points_30.query(\"z <= 600\")\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_10 = gpd.GeoDataFrame(gpd.GeoSeries(points_10.unary_union.convex_hull))\n",
    "    polygon_10 = polygon_10.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "\n",
    "    # Create empty list and append polygons\n",
    "    polygons = []\n",
    "\n",
    "    # Append\n",
    "    polygons.append(polygon_10)\n",
    "    polygons.append(polygon_20)\n",
    "    polygons.append(polygon_30)\n",
    "\n",
    "    # Clip the overlapping distance ploygons (create two donuts + hole)\n",
    "    for i in reversed(range(1, len(distances))):\n",
    "        polygons[i] = gpd.overlay(polygons[i], polygons[i-1], how=\"difference\")\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pharmacy_measure_acc (adjusted to incorporate dijkstra_cca_polygons)\n",
    "\n",
    "Measures the effect of a single pharmacy on the surrounding area. (Uses `dijkstra_cca_polygons`)\n",
    "\n",
    "Args:\n",
    "\n",
    "* pharmacies_df: Geopandas dataframe with information on a pharmacy\n",
    "* distances: Distances in time to calculate accessibility \n",
    "* weights: how to weight the different travel distances\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * Int (\\_thread\\_id)\n",
    "    * GeoDataFrame of catchment areas with key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pharmacy_measure_acc (pharmacies_df, distances, weights):\n",
    "    \n",
    "    # Create polygons\n",
    "    polygons = dijkstra_cca_polygons(G, pharmacies['nearest_osm'], distances)\n",
    "    \n",
    "    # update polygons with time, and ICU beds. Set CRS to 4326, then convert to 6589\n",
    "    for i in range(len(distances)):\n",
    "        polygons[i]['time']=distances[i]\n",
    "        polygons[i].crs = { 'init' : 'epsg:4326'}\n",
    "        polygons[i] = polygons[i].to_crs({'init':'epsg:6589'})\n",
    "    \n",
    "    return([ polygon.copy(deep=True) for polygon in polygons ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### measure_acc_par\n",
    "\n",
    "Parallel implementation of accessibility measurement.\n",
    "\n",
    "Args:\n",
    "\n",
    "* pharmacies: Geodataframe of pharmacies\n",
    "* network: OSMNX street network\n",
    "* distances: list of distances to calculate catchments for\n",
    "* weights: list of floats to apply to different catchments\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe of catchments with accessibility statistics calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_acc_par (pharmacies_df, network, distances, weights):\n",
    "    \n",
    "    # initialize catchment list, 3 empty geodataframes\n",
    "    catchments = []\n",
    "    for distance in distances:\n",
    "        catchments.append(gpd.GeoDataFrame())\n",
    "        \n",
    "    # pool = mp.Pool(processes = num_proc)\n",
    "    \n",
    "    # makes a list of all pharmacy info.  len = 192\n",
    "    # looks like this, except with all info, and for all 192 hospitals\n",
    "    # [[2, Methodist Hospital of Chicago, Chicago], [4, Advocate Christ Medical Center, Oak Lawn]]\n",
    "    pharmacy_list = [ pharmacies_df.iloc[i] for i in range(len(pharmacies_df)) ]\n",
    "    \n",
    "    print(\"Calculating\", len(pharmacy_list), \"pharmacy catchments...\\ncompleted number:\", end=\" \")\n",
    "    \n",
    "    # call pharmacy_acc_unpacker\n",
    "    # returns a tuple containing the thread ID and a list of copied polygons\n",
    "    #results = pool.map(hospital_acc_unpacker, zip(range(len(hospital_list)), hospital_list, itertools.repeat(pop_data), itertools.repeat(distances), itertools.repeat(weights)))\n",
    "    \n",
    "    results = []  \n",
    "    for i in range(len(pharmacy_list)): #do from 1 to 66\n",
    "        result = pharmacy_measure_acc(i, pharmacy_list[i], distances, weights)\n",
    "        results.append(result)\n",
    "\n",
    "    # pool.close()\n",
    "    \n",
    "    # sort and extract the results\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    \n",
    "    # combine catchment results into the respective GeoDataFrames in the catchments list\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(distances)):\n",
    "            catchments[j] = catchments[j].append(results[i][j], sort=False)\n",
    "            \n",
    "    return catchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [600, 1200, 1800] # Distances in travel time (seconds!)\n",
    "weights = [1.0, 0.68, 0.22] \n",
    "\n",
    "# initialize catchment list, 3 empty geodataframes\n",
    "catchments = []\n",
    "for distance in distances:\n",
    "    catchments.append(gpd.GeoDataFrame())\n",
    "    \n",
    "    \n",
    "results = gpd.GeoDataFrame(columns = [\"geometry\",\"pharmid\",\"weight\"], crs = \"EPSG:4326\", geometry = \"geometry\")\n",
    "for ind in pharmacies_osm.index:\n",
    "    print(\"Working on pharmacy\", pharmacies_osm['pharmid'][ind])\n",
    "     ## CREATE DICTIONARIES ##\n",
    "    # create dictionary of nearest nodes\n",
    "    nearest_nodes_30 = nx.single_source_dijkstra_path_length(G, pharmacies_osm['nearest_osm'][ind], distances[2], \"travel_time\") # creating the largest graph from which 10 and 20 minute drive times can be extracted from\n",
    "\n",
    "    # extract values within 20 and 10 (respectively) minutes drive times\n",
    "    nearest_nodes_20 = dict()\n",
    "    nearest_nodes_10 = dict()\n",
    "    for key, value in nearest_nodes_30.items():\n",
    "        if value <= distances[1]:\n",
    "            nearest_nodes_20[key] = value\n",
    "        if value <= distances[0]:\n",
    "            nearest_nodes_10[key] = value\n",
    "\n",
    "    ## CREATE POLYGONS FOR 3 DISTANCE CATEGORIES (10 min, 20 min, 30 min) ##\n",
    "\n",
    "    # 30 MIN\n",
    "    # If the graph already has a geometry attribute with point data,\n",
    "    # this line will create a GeoPandas GeoDataFrame from the nearest_nodes_30 dictionary\n",
    "    points_30 = gpd.GeoDataFrame(gpd.GeoSeries(nx.get_node_attributes(G.subgraph(nearest_nodes_30), 'geometry')))\n",
    "\n",
    "    # This line converts the nearest_nodes_30 dictionary into a Pandas data frame and joins it to points\n",
    "    # left_index=True and right_index=True are options for merge() to join on the index values\n",
    "    points_30 = points_30.merge(pd.Series(nearest_nodes_30).to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "    # Re-name the columns and set the geodataframe geometry to the geometry column\n",
    "    points_30 = points_30.rename(columns={'0_x':'geometry','0_y':'z'}).set_geometry('geometry')\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_30 = gpd.GeoDataFrame(gpd.GeoSeries(points_30.unary_union.convex_hull))\n",
    "    polygon_30 = polygon_30.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    polygon_30[\"weight\"] = weights[2]\n",
    "\n",
    "    # 20 MIN # 1200 seconds!\n",
    "    # Select nodes less than or equal to 20\n",
    "    points_20 = points_30.query(\"z <= 1200\")\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_20 = gpd.GeoDataFrame(gpd.GeoSeries(points_20.unary_union.convex_hull))\n",
    "    polygon_20 = polygon_20.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    polygon_20[\"weight\"] = weights[1]\n",
    "\n",
    "    # 10 MIN # 600 seconds!\n",
    "    # Select nodes less than or equal to 10\n",
    "    points_10 = points_30.query(\"z <= 600\")\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_10 = gpd.GeoDataFrame(gpd.GeoSeries(points_10.unary_union.convex_hull))\n",
    "    polygon_10 = polygon_10.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    polygon_10[\"weight\"] = weights[0]\n",
    "    \n",
    "    # Clip the overlapping distance ploygons (create two donuts + hole)\n",
    "    polygon_30_hole = gpd.overlay(polygon_30, polygon_20, how =\"difference\")\n",
    "    polygon_20_hole = gpd.overlay(polygon_20, polygon_10, how =\"difference\")\n",
    "    \n",
    "    # Create dataframe combining polygon_10, polygon_20, polygon_30\n",
    "    polygons = pd.concat([polygon_10, polygon_20_hole, polygon_30_hole])\n",
    "    polygons.set_crs(crs=\"EPSG:4326\", inplace = True)\n",
    "    # polygons = pd.concat([polygon_10, polygon_20, polygon_30])\n",
    "    # polygons.set_crs(crs=\"EPSG:4326\", inplace = True)\n",
    "    \n",
    "    \n",
    "    polygons_gdf = polygons\n",
    "    polygons_gdf[\"pharmid\"] = pharmacies_osm['pharmid'][ind]\n",
    "    #polygons_gdf[\"weight\"] = weights\n",
    "\n",
    "    results = pd.concat([results, polygons_gdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find for which pharmacies there are issues with geometry and thus their polygon catchments\n",
    "summary_table = results['pharmid'].value_counts().reset_index()\n",
    "display(summary_table) #VT14, MA35, VT74, VT99 only have on geometry row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pharmacies without correct geometries\n",
    "#results_clean = results.drop(results[results['pharmid'] == [\"VT14\",\"MA35\",\"VT74\",\"VT99\"]].index)\n",
    "results_clean = results[~results['pharmid'].isin([\"VT14\",\"MA35\",\"VT74\",\"VT99\"])]\n",
    "results_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change CRS to match \n",
    "results_clean.to_crs(\"EPSG:6589\", inplace = True)\n",
    "pop_df.to_crs(\"EPSG:6589\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate town areas\n",
    "pop_df['town_area'] = pop_df.geometry.area\n",
    "#display(pop_df)\n",
    "\n",
    "#results_clean['s_area'] = results_clean.geometry.area\n",
    "\n",
    "# Run the overlay to find intersection of fragments\n",
    "fragments = gpd.overlay(pop_df, results_clean, how = 'intersection')\n",
    "\n",
    "# Calculate fragment areas\n",
    "fragments['frag_area'] = fragments.geometry.area\n",
    "\n",
    "# Calculate area ratios\n",
    "fragments['area_ratio']= fragments['frag_area'] / fragments['town_area']\n",
    "\n",
    "# Calculate fragment value by multiplying area_ratio by distance weight\n",
    "fragments['value'] = fragments['weight'] * fragments['area_ratio']\n",
    "\n",
    "fragments.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_by pharmid and GEOID, then sum values to summarize the fragment values by pharmid\n",
    "sum_results = fragments.groupby(by = ['pharmid','GEOID']).sum()\n",
    "sum_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate population served in each fragment\n",
    "sum_results['pop_fragment'] = sum_results['value'] * sum_results['total_pop']\n",
    "sum_results.head()\n",
    "\n",
    "# Convert pharmacies dataset to EPSG 6589 to match other datasets\n",
    "pharmacies_df.to_crs(\"EPSG:6589\", inplace = True)\n",
    "\n",
    "# Calculate service to population ratio for each fragment\n",
    "accessibility_df = sum_results.merge(pharmacies_df, on='pharmid')\n",
    "accessibility_df['week_staff'] = accessibility_df['week_pharm'] + 0.5 * accessibility_df['week_tech']\n",
    "accessibility_df['serv_pop_week'] = accessibility_df['pop_fragment'] / accessibility_df['week_staff']\n",
    "accessibility_df.head()\n",
    "\n",
    "# Multiply by weight (again) and summarize by town \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pharmacies_df.to_crs(\"EPSG:6589\", inplace = True)\n",
    "pharmacies_df.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlapping_function\n",
    "\n",
    "Calculates how all catchment areas overlap with and affect the accessibility of each grid in our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* vt_towns: GeoDataFrame of our grid (VT towns)\n",
    "* catchments: GeoDataFrame of our catchments\n",
    "* service_type: the kind of care being provided (ICU beds vs. ventilators) XX\n",
    "* weights: the weight to apply to each service type\n",
    "* num\\_proc: the number of processors\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe - grid\\_file with calculated stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make \"grid file\" of VT towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_function (vt_towns, catchments, service_type, weights, num_proc = 4):\n",
    "\n",
    "    ## Area Weighted Reaggregation\n",
    "        # switch to True for area-weighted overlay\n",
    "    weighted = True\n",
    "\n",
    "    # if the value to be calculated is already in the hegaxon grid, delete it\n",
    "    # otherwise, the field name gets a suffix _1 in the overlay step\n",
    "    if resource in list(vt_towns.columns.values):\n",
    "        vt_towns = vt_towns.drop(resource, axis = 1)\n",
    "\n",
    "    # calculate hexagon 'target' areas\n",
    "    vt_towns['area'] = vt_towns.area\n",
    "\n",
    "    # Intersection overlay of pharmacy catchments and towns grid\n",
    "    print(\"Intersecting pharmacy catchments with town grid...\")\n",
    "    fragments = gpd.overlay(vt_towns, geocatchments, how='intersection')\n",
    "\n",
    "    # Calculate percent coverage of the hexagon by the hospital catchment as\n",
    "    # fragment area / target(hexagon) area\n",
    "    fragments['percent'] = fragments.area / fragments['area']\n",
    "\n",
    "    # if using weighted aggregation... \n",
    "    if weighted:\n",
    "        print(\"Calculating area-weighted value...\")\n",
    "        # multiply the service/population ratio by the distance weight and the percent coverage\n",
    "        fragments['value'] = fragments[resource] * fragments['weight'] * fragments['percent']\n",
    "\n",
    "    # if using the 50% coverage rule for unweighted aggregation...\n",
    "    # else:\n",
    "    #     print(\"Calculating value for hexagons with >=50% overlap...\")\n",
    "    #     # filter for only the fragments with > 50% coverage by hospital catchment\n",
    "    #     fragments = fragments[fragments['percent']>=0.5]\n",
    "    #     # multiply the service/population ration by the distance weight\n",
    "    #     fragments['value'] = fragments[resource] * fragments['weight']\n",
    "\n",
    "    # select just the hexagon id and value from the fragments,\n",
    "    # group the fragments by the (hexagon) id,\n",
    "    # and sum the values\n",
    "    print(\"Summarizing results by town id...\")\n",
    "    sum_results = fragments[['id', 'value']].groupby(by = ['id']).sum()\n",
    "\n",
    "    # join the results to the hexagon grid_file based on hexagon id\n",
    "    print(\"Joining results to hexagons...\")\n",
    "    results = pd.merge(vt_towns, sum_results, how=\"left\", on = \"id\")\n",
    "\n",
    "    # rename value column name to the resource name \n",
    "    return(results.rename(columns = {'value' : resource}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization\n",
    "\n",
    "Normalizes our result (Geodataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (result, resource):\n",
    "    result[resource]=(result[resource]-min(result[resource]))/(max(result[resource])-min(result[resource]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_import\n",
    "\n",
    "Imports all files we need to run our code and pulls the Illinois network from OSMNX if it is not present (will take a while). \n",
    "\n",
    "**NOTE:** even if we calculate accessibility for just Chicago, we want to use the Illinois network (or at least we should not use the Chicago network) because using the Chicago network will result in hospitals near but outside of Chicago having an infinite distance (unreachable because roads do not extend past Chicago).\n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_type: population type, either \"pop\" for general population or \"covid\" for COVID-19 cases\n",
    "* region: the region to use for our hospital and grid file (\"Chicago\" or \"Illinois\")\n",
    "\n",
    "Returns:\n",
    "\n",
    "* G: OSMNX network\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* grid_file: Geodataframe of grids\n",
    "* pop_data: Geodataframe of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_map(output_grid, base_map, hospitals, resource):\n",
    "    ax=output_grid.plot(column=resource, cmap='PuBuGn',figsize=(18,12), legend=True, zorder=1)\n",
    "    # Next two lines set bounds for our x- and y-axes because it looks like there's a weird \n",
    "    # Point at the bottom left of the map that's messing up our frame (Maja)\n",
    "    ax.set_xlim([314000, 370000])\n",
    "    ax.set_ylim([540000, 616000])\n",
    "    base_map.plot(ax=ax, facecolor=\"none\", edgecolor='gray', lw=0.1)\n",
    "    hospitals.plot(ax=ax, markersize=10, zorder=1, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "Below you can customize the input of the model:\n",
    "\n",
    "* Processor - the number of processors to use\n",
    "* Population - the population to calculate the measure for\n",
    "* Resource - the hospital resource of interest\n",
    "* Hospital - all hospitals or subset to check code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "To simplify the reanalysis, in variables I will hardcode the use of \n",
    "    4 processors\n",
    "    Population: Population at Risk\n",
    "    Resource: ICU Beds\n",
    "    Hospital: All hospitals\n",
    "'''\n",
    "\n",
    "resource = \"hospital_icu_beds\"\n",
    "num_proc = 4\n",
    "pop_type = \"pop\"\n",
    "\n",
    "## Create centroids for atrisk population at the census tract level\n",
    "pop_data = pop_centroid(atrisk_data, pop_type)    \n",
    "    \n",
    "distances = [600, 1200, 1800] # Distances in travel time (seconds!)\n",
    "weights = [1.0, 0.68, 0.22] # Weights where weights[0] is applied to distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hospital data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the nearest network node for each hospital\n",
    "hospitals = hospital_setting(hospitals, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize catchment areas for hospital #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point geometries for entire graph\n",
    "\n",
    "# which hospital to visualize? \n",
    "fighosp = 4\n",
    "\n",
    "# Create catchment for hospital 4\n",
    "poly = dijkstra_cca_polygons(G, hospitals['nearest_osm'][fighosp], distances)\n",
    "\n",
    "# Reproject polygons\n",
    "for i in range(len(poly)):\n",
    "    poly[i].crs = { 'init' : 'epsg:4326'}\n",
    "    poly[i] = poly[i].to_crs({'init':'epsg:32616'})\n",
    "\n",
    "# Reproject hospitals \n",
    "hospital_subset = hospitals.iloc[[fighosp]].to_crs(epsg=32616)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "min_10 = poly[0].plot(ax=ax, color=\"royalblue\", label=\"10 min drive\")\n",
    "min_20 = poly[1].plot(ax=ax, color=\"cornflowerblue\", label=\"20 min drive\")\n",
    "min_30 = poly[2].plot(ax=ax, color=\"lightsteelblue\", label=\"30 min drive\")\n",
    "\n",
    "hospital_subset.plot(ax=ax, color=\"red\", legend=True, label = \"hospital\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate hospital catchment areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "catchments = measure_acc_par(hospitals, pop_data, G, distances, weights, num_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-process the catchments (for area weighted reaggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weight field to each catchment polygon\n",
    "for i in range(len(weights)):\n",
    "    catchments[i]['weight'] = weights[i]\n",
    "# combine the three sets of catchment polygons into one geodataframe\n",
    "geocatchments = pd.concat([catchments[0], catchments[1], catchments[2]])\n",
    "geocatchments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Weighted Reaagregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = overlapping_function(grid_file, catchments, resource, weights, num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = normalization (result, resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Discussion\n",
    "\n",
    "Extensive cleaning of unneccesary variables and lines of code that were never called.\n",
    "\n",
    "### Making code more efficient and easier to read with GeoPandas\n",
    "\n",
    "1. Made the __pop_centroid__ function much faster - preciosly took 3:30 to run, now less than a second. Instead of creating an empty GDF and iterating over all of the population geometries, adding data to this new GDF, I just used the native GeoPandas centroid method, replacing the population geometries with centroids, and then dropping other unnecessary columns from atrisk_data. \n",
    "\n",
    "2. Rewrote the __hospital_setting__ function to find each hospital's nearest node using GeoPandas nearest join method. What took 1:20 to run now runs in less than a second. I also cleaned the GDF so that it matched what we were working with before. \n",
    "\n",
    "### Removed parallel processing from two functions. \n",
    "\n",
    "1. __overlapping_function__\n",
    "2. __measure_acc_par__\n",
    "\n",
    "### Theoretical Changes to the methodology\n",
    "\n",
    "Area weighted reaggregation - \n",
    "assigned speeds to the road network using osnmx. \n",
    "\n",
    "### Simplifying Code for future students\n",
    "\n",
    "My greatest contribution to this replication has been the simplification of code and adding documentation to functions. This has made the code much easier for future students to read through and understand, and has not sacrificed processing times. I also made a visual workflow, visualizing the replication study from start to finish, including all data and functions used to manipulate them. \n",
    "\n",
    "Simplifications include:\n",
    "\n",
    "I removed the dropdown menu that allows you to choose between population groups and hospital data. The benefits of this dropdown options were minimal, and it just made the code more confusing to follow and modify. In the form of a dropdown selection, it prevents the study from being one script, and introduces potential error as groups try to replicate eachother, if they are not clear about which choices they made with their mouse in the dropdown. \n",
    "\n",
    "I was able to delete the function __overlap_calc__, after implementing its function into __overlapping_function__ which was implements the area weighted reaggregation.\n",
    "\n",
    "I removed a code block that filtered rows where the \"hospital_icu_beds\" value is infinity, which did not do anything. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accessibility Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hospitals = hospitals.to_crs({'init': 'epsg:26971'})\n",
    "result = result.to_crs({'init': 'epsg:26971'})\n",
    "output_map(result, pop_data, hospitals, resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classified Accessibility Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "to be written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Luo, W., & Qi, Y. (2009). An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians. Health & place, 15(4), 1100-1107."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
